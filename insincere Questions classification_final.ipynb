{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":10737,"databundleVersionId":290346,"isSourceIdPinned":false,"sourceType":"competition"}],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport json\nfrom transformers import AutoTokenizer, AutoModel\nfrom torch.utils.data import DataLoader, Dataset\nfrom tqdm import tqdm\nimport os\nfrom sklearn.metrics import f1_score\nfrom sklearn.model_selection import train_test_split\nimport wandb\nfrom torch.optim.lr_scheduler import ExponentialLR","metadata":{"execution":{"iopub.status.busy":"2025-04-03T12:34:25.889595Z","iopub.execute_input":"2025-04-03T12:34:25.889935Z","iopub.status.idle":"2025-04-03T12:34:30.295990Z","shell.execute_reply.started":"2025-04-03T12:34:25.889899Z","shell.execute_reply":"2025-04-03T12:34:30.295208Z"},"trusted":true},"outputs":[],"execution_count":1},{"cell_type":"code","source":"MODEL_NAME = \"FacebookAI/roberta-base\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-03T12:34:30.296987Z","iopub.execute_input":"2025-04-03T12:34:30.297213Z","iopub.status.idle":"2025-04-03T12:34:30.300574Z","shell.execute_reply.started":"2025-04-03T12:34:30.297191Z","shell.execute_reply":"2025-04-03T12:34:30.299779Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"from kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()\nsecret_value_0 = user_secrets.get_secret(\"WANDB_API_KEY\")\nwandb.login(key = secret_value_0)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-03T12:34:30.302264Z","iopub.execute_input":"2025-04-03T12:34:30.302493Z","iopub.status.idle":"2025-04-03T12:34:36.819541Z","shell.execute_reply.started":"2025-04-03T12:34:30.302473Z","shell.execute_reply":"2025-04-03T12:34:36.818836Z"}},"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mshobhitshukla6535\u001b[0m (\u001b[33mshobhitshukla6535-iit-kharagpur\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n","output_type":"stream"},{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"True"},"metadata":{}}],"execution_count":3},{"cell_type":"code","source":"class QuestionsDataset(Dataset):\n    def __init__(self, dataset, tokenizer, is_test=False, max_length=62):\n        self.dataset = dataset\n        self.tokenizer = tokenizer\n        self.max_length = max_length\n        self.is_test = is_test\n\n    def __len__(self):\n        return len(self.dataset)\n\n    def __getitem__(self, idx):\n        text = str(self.dataset.iloc[idx, 1])\n\n        if not self.is_test:\n            target = self.dataset.iloc[idx, 2]\n            # assert isinstance(target, np.int16)\n        encoding = self.tokenizer.encode_plus(\n            text,\n            max_length=self.max_length,\n            padding=\"max_length\",\n            truncation=True,\n            add_special_tokens=True,\n            return_tensors=\"pt\",\n        )\n\n        if self.is_test:\n            return {\n                \"input_ids\": encoding[\"input_ids\"].squeeze(),\n                \"attention_mask\": encoding[\"attention_mask\"].squeeze(),\n            }\n\n        else:\n            return {\n                \"input_ids\": encoding[\"input_ids\"].squeeze(),\n                \"attention_mask\": encoding[\"attention_mask\"].squeeze(),\n                \"targets\": torch.FloatTensor([target]),\n            }","metadata":{"execution":{"iopub.status.busy":"2025-04-03T12:34:36.820577Z","iopub.execute_input":"2025-04-03T12:34:36.821060Z","iopub.status.idle":"2025-04-03T12:34:36.826610Z","shell.execute_reply.started":"2025-04-03T12:34:36.821020Z","shell.execute_reply":"2025-04-03T12:34:36.825962Z"},"trusted":true},"outputs":[],"execution_count":4},{"cell_type":"code","source":"class BERT_MODEL(nn.Module):\n    def __init__(self, model_name):\n        super(BERT_MODEL, self).__init__()\n        self.model = AutoModel.from_pretrained(model_name)\n        self.linear = nn.Sequential(\n            nn.Linear(768, 1024), nn.ReLU(), nn.Dropout(0.3), nn.Linear(1024, 1)\n        )\n\n    def forward(self, input_ids, attention_mask):\n        output = self.model(input_ids=input_ids, attention_mask=attention_mask)\n        pooled_output = output.pooler_output\n        output = self.linear(pooled_output)\n        return output","metadata":{"execution":{"iopub.status.busy":"2025-04-03T12:34:36.827455Z","iopub.execute_input":"2025-04-03T12:34:36.827726Z","iopub.status.idle":"2025-04-03T12:34:36.843716Z","shell.execute_reply.started":"2025-04-03T12:34:36.827706Z","shell.execute_reply":"2025-04-03T12:34:36.842982Z"},"trusted":true},"outputs":[],"execution_count":5},{"cell_type":"code","source":"class Trainer:\n    def __init__(\n        self,\n        dataset_path,\n        model_card,\n        checkpoint_dir=\"checkpoints\",\n        device=\"cuda\" if torch.cuda.is_available() else \"cpu\",\n        pos_weight=[1, 70],\n        length_percentile=99.9,\n        batch_size=32,\n        epochs=2,\n        seed=42,\n    ):\n        self.device = device\n        self.batch_size = batch_size\n        self.epochs = epochs\n        self.seed = seed\n        print(f\"Using {self.device} device\")\n        self.model_name = model_card\n        self.get_dataloader(dataset_path, length_percentile)\n        print(f\"Train dataset size: {len(self.train_dl.dataset)}\")\n        print(f\"Validation dataset size: {len(self.val_dl.dataset)}\")\n        self.model = BERT_MODEL(self.model_name)\n        print(f\"Model: {self.model_name} initialized\")\n        self.num_epochs = epochs\n        self.criterion = nn.BCEWithLogitsLoss()\n        self.checkpoint_dir = checkpoint_dir\n        os.makedirs(checkpoint_dir, exist_ok=True)\n        self.optimizer = optim.AdamW(self.model.parameters(), lr=1e-5)\n        self.scheduler = ExponentialLR(self.optimizer, gamma=0.9)\n\n        wandb.init(\n            project=\"question-classification\",\n            config={\n                \"epochs\": self.epochs,\n                \"batch_size\": self.batch_size,\n                \"learning_rate\": 1e-5,\n                \"model\": self.model_name,\n            },\n        )\n\n    @staticmethod\n    def get_max_length(df_train, tokenizer, length_percentile=99.9):\n        df_train[\"question_length\"] = tokenizer(\n            df_train.question_text.tolist(), truncation=True\n        )[\"input_ids\"]\n        df_train[\"question_length\"] = df_train[\"question_length\"].apply(\n            lambda x: len(x)\n        )\n        max_length = np.percentile(df_train[\"question_length\"], length_percentile)\n\n        return int(max_length)\n\n    def get_dataloader(self, path, length_percentile=99.9):\n        df_train = pd.read_csv(path)\n        df_train.target = df_train.target.astype(\"int16\")\n        # df_train = df_train.iloc[:100,:]\n        tokenizer = AutoTokenizer.from_pretrained(self.model_name, use_fast=True)\n        max_length = 62  # self.get_max_length(df_train, tokenizer, length_percentile)\n        train_df, val_df = train_test_split(\n            df_train,\n            stratify=df_train.target,\n            test_size=0.2,\n            random_state=42,\n        )\n\n        train_ds = QuestionsDataset(\n            train_df, tokenizer, is_test=False, max_length=max_length\n        )\n        val_ds = QuestionsDataset(\n            val_df, tokenizer, is_test=False, max_length=max_length\n        )\n\n        self.train_dl = DataLoader(train_ds, batch_size=self.batch_size, shuffle=True)\n        self.val_dl = DataLoader(val_ds, batch_size=self.batch_size)\n\n    @staticmethod\n    def find_best_f1(outputs, labels):\n        tmp = [0, 0, 0]  # idx, current, max\n        threshold = 0\n\n        for tmp[0] in np.arange(0.25, 0.85, 0.01):\n            tmp[1] = f1_score(labels, outputs > tmp[0], zero_division=0)\n            if tmp[1] > tmp[2]:\n                threshold = tmp[0]\n                tmp[2] = tmp[1]\n\n        return tmp[2], threshold\n\n    @staticmethod\n    def get_preds(logits, threshold):\n        \"\"\"Convert logits to binary predictions based on the threshold\"\"\"\n        predictions = (torch.sigmoid(logits) > threshold).float()\n        return predictions\n\n    def train(self):\n        torch.manual_seed(self.seed)\n        self.model.to(self.device)\n        history = {\"Train_Loss\": [], \"Val_Loss\": [], \"F1\": []}\n        agg_loss = 0\n        step = 0\n        for epoch in range(self.num_epochs):\n            print(\"*\" * 10 + f\" Epoch - {epoch + 1} \" + \"*\" * 10)\n            progress_bar = tqdm(\n                self.train_dl,\n                desc=\"Optimizing\",\n                unit=\"batch\",\n                leave=True,\n                dynamic_ncols=True,\n            )\n\n            for i, batch in enumerate(progress_bar):\n                input_ids, attention_mask, targets = (\n                    batch[\"input_ids\"].to(self.device),\n                    batch[\"attention_mask\"].to(self.device),\n                    batch[\"targets\"].to(self.device).squeeze(1),\n                )\n\n                self.optimizer.zero_grad()\n                logits = self.model(input_ids, attention_mask)\n                loss = self.criterion(logits.squeeze(1), targets)\n                agg_loss += loss.item()\n                loss.backward()\n\n                nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)\n\n                self.optimizer.step()\n\n                history[\"Train_Loss\"].append(agg_loss / (step + 1))\n                wandb.log({\"Train Loss\": history[\"Train_Loss\"][-1]}, step=step + 1)\n\n                progress_bar.set_postfix(\n                    {\"Train loss\": history[\"Train_Loss\"][-1]}, refresh=True\n                )\n\n                step = step + 1\n\n                if (step + 1) % 2000 == 0:\n                    checkpoint_path = os.path.join(\n                        self.checkpoint_dir, f\"model_epoch_{epoch + 1}.pth\"\n                    )\n                    self.scheduler.step()\n                    print(f\"Learning Rate after step {step}: {self.scheduler.get_last_lr()}\")\n                    wandb.log({\"last_lr\": self.scheduler.get_last_lr()}, step = step)\n                    \n                    torch.save(\n                        {\n                            \"epoch\": epoch + 1,\n                            \"model_state_dict\": self.model.state_dict(),\n                            \"optimizer_state_dict\": self.optimizer.state_dict(),\n                            \"loss\": history[\"Train_Loss\"][-1],\n                        },\n                        checkpoint_path,\n                    )\n                    print(f\"Checkpoint saved at {checkpoint_path}\")\n                    val_loss, best_f1 = self.validate()\n                    wandb.log({\"Validation Loss\": val_loss, \"F1 Score\": best_f1}, step=step)\n                    history[\"Val_Loss\"].append(val_loss)\n                    history[\"F1\"].append(best_f1)\n            \n        history_path = os.path.join(self.checkpoint_dir, \"training_history.json\")\n        with open(history_path, \"w\") as f:\n            json.dump(history, f)\n    \n        wandb.save(history_path) \n    \n        model_path = os.path.join(self.checkpoint_dir, \"final_model.pth\")\n        torch.save(self.model.state_dict(), model_path)\n    \n        wandb.save(model_path) \n    \n        print(\"Training complete! History and model saved to W&B.\")\n        wandb.finish()\n        return history\n\n    def validate(self):\n        \"\"\"Validation logic separated into its own function.\"\"\"\n        self.model.eval()\n        agg_val_loss = 0\n        all_logits, all_targets = [], []\n\n        with torch.no_grad():\n            for batch in tqdm(self.val_dl, desc=\"Validating\", unit=\"batch\"):\n                input_ids, attention_mask, targets = (\n                    batch[\"input_ids\"].to(self.device),\n                    batch[\"attention_mask\"].to(self.device),\n                    batch[\"targets\"].to(self.device).squeeze(1),\n                )\n\n                logits = self.model(input_ids, attention_mask)\n                loss = self.criterion(logits.squeeze(1), targets)\n                agg_val_loss += loss.item()\n\n                all_logits.append(logits.squeeze(1).cpu())\n                all_targets.append(targets.cpu())\n\n        all_logits = torch.cat(all_logits, dim=0)\n        all_targets = torch.cat(all_targets, dim=0)\n\n        val_loss = agg_val_loss / len(self.val_dl)\n        best_f1, threshold = self.find_best_f1(\n            torch.sigmoid(all_logits).numpy(), all_targets.numpy()\n        )\n\n        print(\n            f\"Validation - Loss: {val_loss:.4f}, F1 Score: {best_f1:.4f} (Threshold: {threshold:.2f})\"\n        )\n\n        return val_loss, best_f1","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-03T12:34:36.844549Z","iopub.execute_input":"2025-04-03T12:34:36.844805Z","iopub.status.idle":"2025-04-03T12:34:36.885555Z","shell.execute_reply.started":"2025-04-03T12:34:36.844785Z","shell.execute_reply":"2025-04-03T12:34:36.884754Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"trainer = Trainer(\n    dataset_path=\"/kaggle/input/quora-insincere-questions-classification/train.csv\",\n    model_card=MODEL_NAME,\n    checkpoint_dir=\"checkpoints\",\n    pos_weight=[1, 20],\n    length_percentile=99.9,\n    batch_size=64,\n    epochs=2\n)","metadata":{"execution":{"iopub.status.busy":"2025-04-03T12:34:36.886356Z","iopub.execute_input":"2025-04-03T12:34:36.886627Z","iopub.status.idle":"2025-04-03T12:34:50.209371Z","shell.execute_reply.started":"2025-04-03T12:34:36.886604Z","shell.execute_reply":"2025-04-03T12:34:50.208674Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Using cuda device\nTrain dataset size: 1044897\nValidation dataset size: 261225\n","output_type":"stream"},{"name":"stderr","text":"Some weights of RobertaModel were not initialized from the model checkpoint at FacebookAI/roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"Model: FacebookAI/roberta-base initialized\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.19.1"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20250403_123443-7ll8wh6h</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/shobhitshukla6535-iit-kharagpur/question-classification/runs/7ll8wh6h' target=\"_blank\">blooming-dawn-13</a></strong> to <a href='https://wandb.ai/shobhitshukla6535-iit-kharagpur/question-classification' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/shobhitshukla6535-iit-kharagpur/question-classification' target=\"_blank\">https://wandb.ai/shobhitshukla6535-iit-kharagpur/question-classification</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/shobhitshukla6535-iit-kharagpur/question-classification/runs/7ll8wh6h' target=\"_blank\">https://wandb.ai/shobhitshukla6535-iit-kharagpur/question-classification/runs/7ll8wh6h</a>"},"metadata":{}}],"execution_count":7},{"cell_type":"code","source":"history = trainer.train()","metadata":{"trusted":true,"scrolled":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}
{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-03T12:34:25.889935Z",
     "iopub.status.busy": "2025-04-03T12:34:25.889595Z",
     "iopub.status.idle": "2025-04-03T12:34:30.295990Z",
     "shell.execute_reply": "2025-04-03T12:34:30.295208Z",
     "shell.execute_reply.started": "2025-04-03T12:34:25.889899Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import json\n",
    "from rich.progress import Progress, TextColumn, BarColumn, SpinnerColumn, MofNCompleteColumn\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "import wandb\n",
    "from torch.optim.lr_scheduler import ExponentialLR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-03T12:34:30.297213Z",
     "iopub.status.busy": "2025-04-03T12:34:30.296987Z",
     "iopub.status.idle": "2025-04-03T12:34:30.300574Z",
     "shell.execute_reply": "2025-04-03T12:34:30.299779Z",
     "shell.execute_reply.started": "2025-04-03T12:34:30.297191Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "MODEL_NAME = \"FacebookAI/roberta-base\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-03T12:34:30.302493Z",
     "iopub.status.busy": "2025-04-03T12:34:30.302264Z",
     "iopub.status.idle": "2025-04-03T12:34:36.819541Z",
     "shell.execute_reply": "2025-04-03T12:34:36.818836Z",
     "shell.execute_reply.started": "2025-04-03T12:34:30.302473Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Currently logged in as: shobhitshukla6535 (shobhitshukla6535-iit-kharagpur) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# from kaggle_secrets import UserSecretsClient\n",
    "# user_secrets = UserSecretsClient()\n",
    "# secret_value_0 = user_secrets.get_secret(\"WANDB_API_KEY\")\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-03T12:34:36.821060Z",
     "iopub.status.busy": "2025-04-03T12:34:36.820577Z",
     "iopub.status.idle": "2025-04-03T12:34:36.826610Z",
     "shell.execute_reply": "2025-04-03T12:34:36.825962Z",
     "shell.execute_reply.started": "2025-04-03T12:34:36.821020Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class QuestionsDataset(Dataset):\n",
    "    def __init__(self, dataset, tokenizer, is_test=False, max_length=62):\n",
    "        self.dataset = dataset\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        self.is_test = is_test\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = str(self.dataset.iloc[idx, 1])\n",
    "\n",
    "        if not self.is_test:\n",
    "            target = self.dataset.iloc[idx, 2]\n",
    "            # assert isinstance(target, np.int16)\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            max_length=self.max_length,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            add_special_tokens=True,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "\n",
    "        if self.is_test:\n",
    "            return {\n",
    "                \"input_ids\": encoding[\"input_ids\"].squeeze(),\n",
    "                \"attention_mask\": encoding[\"attention_mask\"].squeeze(),\n",
    "            }\n",
    "\n",
    "        else:\n",
    "            return {\n",
    "                \"input_ids\": encoding[\"input_ids\"].squeeze(),\n",
    "                \"attention_mask\": encoding[\"attention_mask\"].squeeze(),\n",
    "                \"targets\": torch.FloatTensor([target]),\n",
    "            }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-03T12:34:36.827726Z",
     "iopub.status.busy": "2025-04-03T12:34:36.827455Z",
     "iopub.status.idle": "2025-04-03T12:34:36.843716Z",
     "shell.execute_reply": "2025-04-03T12:34:36.842982Z",
     "shell.execute_reply.started": "2025-04-03T12:34:36.827706Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class BERT_MODEL(nn.Module):\n",
    "    def __init__(self, model_name):\n",
    "        super(BERT_MODEL, self).__init__()\n",
    "        self.model = AutoModel.from_pretrained(model_name)\n",
    "        self.linear = nn.Sequential(\n",
    "            nn.Linear(768, 1024), nn.ReLU(), nn.Dropout(0.3), nn.Linear(1024, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        output = self.model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        pooled_output = output.pooler_output\n",
    "        output = self.linear(pooled_output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-03T12:34:36.844805Z",
     "iopub.status.busy": "2025-04-03T12:34:36.844549Z",
     "iopub.status.idle": "2025-04-03T12:34:36.885555Z",
     "shell.execute_reply": "2025-04-03T12:34:36.884754Z",
     "shell.execute_reply.started": "2025-04-03T12:34:36.844785Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class Trainer:\n",
    "    def __init__(\n",
    "        self,\n",
    "        dataset_path,\n",
    "        model_card,\n",
    "        checkpoint_dir=\"checkpoints\",\n",
    "        device=\"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
    "        pos_weight=[1, 70],\n",
    "        length_percentile=99.9,\n",
    "        batch_size=32,\n",
    "        epochs=2,\n",
    "        seed=42,\n",
    "    ):\n",
    "        self.device = device\n",
    "        self.batch_size = batch_size\n",
    "        self.epochs = epochs\n",
    "        self.seed = seed\n",
    "        print(f\"Using {self.device} device\")\n",
    "        self.model_name = model_card\n",
    "        self.get_dataloader(dataset_path, length_percentile)\n",
    "        print(f\"Train dataset size: {len(self.train_dl.dataset)}\")\n",
    "        print(f\"Validation dataset size: {len(self.val_dl.dataset)}\")\n",
    "        self.model = BERT_MODEL(self.model_name)\n",
    "        print(f\"Model: {self.model_name} initialized\")\n",
    "        self.num_epochs = epochs\n",
    "        self.criterion = nn.BCEWithLogitsLoss()\n",
    "        self.checkpoint_dir = checkpoint_dir\n",
    "        os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "        self.optimizer = optim.AdamW(self.model.parameters(), lr=1e-5)\n",
    "        self.scheduler = ExponentialLR(self.optimizer, gamma=0.9)\n",
    "\n",
    "        wandb.init(\n",
    "            project=\"question-classification\",\n",
    "            config={\n",
    "                \"epochs\": self.epochs,\n",
    "                \"batch_size\": self.batch_size,\n",
    "                \"learning_rate\": 1e-5,\n",
    "                \"model\": self.model_name,\n",
    "            },\n",
    "        )\n",
    "\n",
    "    @staticmethod\n",
    "    def get_max_length(df_train, tokenizer, length_percentile=99.9):\n",
    "        df_train[\"question_length\"] = tokenizer(\n",
    "            df_train.question_text.tolist(), truncation=True\n",
    "        )[\"input_ids\"]\n",
    "        df_train[\"question_length\"] = df_train[\"question_length\"].apply(\n",
    "            lambda x: len(x)\n",
    "        )\n",
    "        max_length = np.percentile(df_train[\"question_length\"], length_percentile)\n",
    "\n",
    "        return int(max_length)\n",
    "\n",
    "    def get_dataloader(self, path, length_percentile=99.9):\n",
    "        df_train = pd.read_csv(path)\n",
    "        df_train.target = df_train.target.astype(\"int16\")\n",
    "        # df_train = df_train.iloc[:100,:]\n",
    "        tokenizer = AutoTokenizer.from_pretrained(self.model_name, use_fast=True)\n",
    "        max_length = 62  # self.get_max_length(df_train, tokenizer, length_percentile)\n",
    "        train_df, val_df = train_test_split(\n",
    "            df_train,\n",
    "            stratify=df_train.target,\n",
    "            test_size=0.2,\n",
    "            random_state=42,\n",
    "        )\n",
    "\n",
    "        train_ds = QuestionsDataset(\n",
    "            train_df, tokenizer, is_test=False, max_length=max_length\n",
    "        )\n",
    "        val_ds = QuestionsDataset(\n",
    "            val_df, tokenizer, is_test=False, max_length=max_length\n",
    "        )\n",
    "\n",
    "        self.train_dl = DataLoader(train_ds, batch_size=self.batch_size, shuffle=True)\n",
    "        self.val_dl = DataLoader(val_ds, batch_size=self.batch_size)\n",
    "\n",
    "    @staticmethod\n",
    "    def find_best_f1(outputs, labels):\n",
    "        tmp = [0, 0, 0]  # idx, current, max\n",
    "        threshold = 0\n",
    "\n",
    "        for tmp[0] in np.arange(0.25, 0.85, 0.01):\n",
    "            tmp[1] = f1_score(labels, outputs > tmp[0], zero_division=0)\n",
    "            if tmp[1] > tmp[2]:\n",
    "                threshold = tmp[0]\n",
    "                tmp[2] = tmp[1]\n",
    "\n",
    "        return tmp[2], threshold\n",
    "\n",
    "    def train(self):\n",
    "        torch.manual_seed(self.seed)\n",
    "        self.model.to(self.device)\n",
    "        history = {\"Train_Loss\": [], \"Val_Loss\": [], \"F1\": []}\n",
    "        agg_loss = 0\n",
    "        step = 0\n",
    "\n",
    "        progress = Progress(\n",
    "            TextColumn(\"[bold blue]{task.description}\"),\n",
    "            SpinnerColumn(),\n",
    "            BarColumn(),\n",
    "            MofNCompleteColumn(),\n",
    "            TextColumn(\"[progress.percentage]{task.percentage:>3.0f}%\"),\n",
    "        )\n",
    "\n",
    "        epoch_progress = progress.add_task(\n",
    "            \"[cyan] Epochs...: \", total=self.num_epochs, start=True\n",
    "        )\n",
    "\n",
    "        for epoch in range(self.num_epochs):\n",
    "            progress.update(epoch_progress, advance=1)\n",
    "            batch_progress = progress.add_task(\n",
    "                \"[blue] Training :\", total=len(self.train_dl)\n",
    "            )\n",
    "\n",
    "            for i, batch in enumerate(self.train_dl):\n",
    "                input_ids, attention_mask, targets = (\n",
    "                    batch[\"input_ids\"].to(self.device),\n",
    "                    batch[\"attention_mask\"].to(self.device),\n",
    "                    batch[\"targets\"].to(self.device).squeeze(1),\n",
    "                )\n",
    "\n",
    "                self.optimizer.zero_grad()\n",
    "                logits = self.model(input_ids, attention_mask)\n",
    "                loss = self.criterion(logits.squeeze(1), targets)\n",
    "                agg_loss += loss.item()\n",
    "                loss.backward()\n",
    "\n",
    "                nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)\n",
    "\n",
    "                self.optimizer.step()\n",
    "\n",
    "                history[\"Train_Loss\"].append(agg_loss / (step + 1))\n",
    "                wandb.log({\"Train Loss\": history[\"Train_Loss\"][-1]}, step=step + 1)\n",
    "\n",
    "                progress.update(\n",
    "                    batch_progress,\n",
    "                    advance=1,\n",
    "                    description=f\"Train loss: {history['Train_Loss'][-1]:.4f}\",\n",
    "                )\n",
    "\n",
    "                step = step + 1\n",
    "\n",
    "                if (step + 1) % 2000 == 0:\n",
    "                    checkpoint_path = os.path.join(\n",
    "                        self.checkpoint_dir, f\"model_epoch_{epoch + 1}.pth\"\n",
    "                    )\n",
    "                    self.scheduler.step()\n",
    "\n",
    "                    wandb.log({\"last_lr\": self.scheduler.get_last_lr()[0]}, step=step)\n",
    "\n",
    "                    torch.save(\n",
    "                        {\n",
    "                            \"epoch\": epoch + 1,\n",
    "                            \"model_state_dict\": self.model.state_dict(),\n",
    "                            \"optimizer_state_dict\": self.optimizer.state_dict(),\n",
    "                            \"loss\": history[\"Train_Loss\"][-1],\n",
    "                        },\n",
    "                        checkpoint_path,\n",
    "                    )\n",
    "                    progress.console.print(f\"Checkpoint saved at {checkpoint_path}\")\n",
    "                    val_loss, best_f1, threshold = self.validate(progress)\n",
    "                    progress.console.print(\n",
    "                        f\"Learning Rate after {step} steps: {self.scheduler.get_last_lr()[0]} \\n\",\n",
    "                        f\"Train loss: {history['Train_Loss'][-1]:.4f} \\n\",\n",
    "                        f\"Validation Loss: {val_loss}, Best F1 Score: {best_f1}, Threshold: {threshold}\\n\",\n",
    "                    )\n",
    "                    wandb.log(\n",
    "                        {\n",
    "                            \"Validation Loss\": val_loss,\n",
    "                            \"F1 Score\": best_f1,\n",
    "                            \"at_threshold\": threshold,\n",
    "                        },\n",
    "                        step=step,\n",
    "                    )\n",
    "                    history[\"Val_Loss\"].append(val_loss)\n",
    "                    history[\"F1\"].append(best_f1)\n",
    "\n",
    "            progress.remove_task(batch_progress)\n",
    "\n",
    "        history_path = os.path.join(self.checkpoint_dir, \"training_history.json\")\n",
    "        with open(history_path, \"w\") as f:\n",
    "            json.dump(history, f)\n",
    "\n",
    "        wandb.save(history_path)\n",
    "\n",
    "        model_path = os.path.join(self.checkpoint_dir, \"final_model.pth\")\n",
    "        torch.save(self.model.state_dict(), model_path)\n",
    "\n",
    "        wandb.save(model_path)\n",
    "\n",
    "        print(\"Training complete! History and model saved to W&B.\")\n",
    "        wandb.finish()\n",
    "        return history\n",
    "\n",
    "    def validate(self, progress):\n",
    "        \"\"\"Validation logic separated into its own function.\"\"\"\n",
    "        self.model.eval()\n",
    "        agg_val_loss = 0\n",
    "        all_logits, all_targets = [], []\n",
    "        val_progress = progress.add_task(\"[Red] Validating :\", total=len(self.val_dl))\n",
    "        with torch.no_grad():\n",
    "            for i, batch in enumerate(self.val_dl):\n",
    "                input_ids, attention_mask, targets = (\n",
    "                    batch[\"input_ids\"].to(self.device),\n",
    "                    batch[\"attention_mask\"].to(self.device),\n",
    "                    batch[\"targets\"].to(self.device).squeeze(1),\n",
    "                )\n",
    "\n",
    "                logits = self.model(input_ids, attention_mask)\n",
    "                loss = self.criterion(logits.squeeze(1), targets)\n",
    "                agg_val_loss += loss.item()\n",
    "                progress.update(\n",
    "                    val_progress,\n",
    "                    advance=1,\n",
    "                    description=f\"Validated {i + 1} Batches\",\n",
    "                )\n",
    "                all_logits.append(logits.squeeze(1).cpu())\n",
    "                all_targets.append(targets.cpu())\n",
    "\n",
    "        all_logits = torch.cat(all_logits, dim=0)\n",
    "        all_targets = torch.cat(all_targets, dim=0)\n",
    "\n",
    "        val_loss = agg_val_loss / len(self.val_dl)\n",
    "        best_f1, threshold = self.find_best_f1(\n",
    "            torch.sigmoid(all_logits).numpy(), all_targets.numpy()\n",
    "        )\n",
    "\n",
    "        return val_loss, best_f1, threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-03T12:34:36.886627Z",
     "iopub.status.busy": "2025-04-03T12:34:36.886356Z",
     "iopub.status.idle": "2025-04-03T12:34:50.209371Z",
     "shell.execute_reply": "2025-04-03T12:34:50.208674Z",
     "shell.execute_reply.started": "2025-04-03T12:34:36.886604Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    dataset_path=\"/kaggle/input/quora-insincere-questions-classification/train.csv\",\n",
    "    model_card=MODEL_NAME,\n",
    "    checkpoint_dir=\"checkpoints\",\n",
    "    pos_weight=[1, 20],\n",
    "    length_percentile=99.9,\n",
    "    batch_size=64,\n",
    "    epochs=2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "trusted": true
   },
   "outputs": [],
   "source": [
    "history = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "92c24fdbfd4441178e02f580be68550a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Working on epoch #0\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Working on epoch #0\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Working on epoch #1\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Working on epoch #1\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Working on epoch #2\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Working on epoch #2\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[20]\u001b[39m\u001b[32m, line 49\u001b[39m\n\u001b[32m     45\u001b[39m             progress.remove_task(batch_task)\n\u001b[32m     48\u001b[39m \u001b[38;5;66;03m# Run training\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m49\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[20]\u001b[39m\u001b[32m, line 38\u001b[39m, in \u001b[36mtrain\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     35\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mWorking on epoch #\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     37\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[32m1\u001b[39m, total_batches + \u001b[32m1\u001b[39m):\n\u001b[32m---> \u001b[39m\u001b[32m38\u001b[39m     sleep(\u001b[32m0.01\u001b[39m)  \u001b[38;5;66;03m# Simulate training step\u001b[39;00m\n\u001b[32m     39\u001b[39m     progress.update(batch_task, advance=\u001b[32m1\u001b[39m)\n\u001b[32m     41\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m batch % validate_every == \u001b[32m0\u001b[39m:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "from time import sleep\n",
    "from rich.progress import Progress, SpinnerColumn, BarColumn, TextColumn\n",
    "\n",
    "\n",
    "# Validation function with progress bar\n",
    "def validate(progress, total_batches=50):\n",
    "    validation_task = progress.add_task(\"[magenta]Validating...\", total=total_batches)\n",
    "\n",
    "    for _ in range(total_batches):\n",
    "        sleep(0.01)  # Simulate validation step\n",
    "        progress.update(validation_task, advance=1)\n",
    "\n",
    "    progress.remove_task(validation_task)\n",
    "\n",
    "\n",
    "# Training function with integrated progress bars\n",
    "def train():\n",
    "    epochs = 3\n",
    "    total_batches = 500\n",
    "    validate_every = 50\n",
    "    validation_batches = 50\n",
    "\n",
    "    with Progress(\n",
    "        TextColumn(\"[bold blue]{task.description}\"),\n",
    "        BarColumn(),\n",
    "        TextColumn(\"[progress.percentage]{task.percentage:>3.0f}%\"),\n",
    "    ) as progress:\n",
    "        epoch_task = progress.add_task(\"[green]Epoch Progress\", total=epochs)\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            batch_task = progress.add_task(\n",
    "                \"[cyan]Training Batches\", total=total_batches\n",
    "            )\n",
    "\n",
    "            print(f\"Working on epoch #{epoch}\")\n",
    "\n",
    "            for batch in range(1, total_batches + 1):\n",
    "                sleep(0.01)  # Simulate training step\n",
    "                progress.update(batch_task, advance=1)\n",
    "\n",
    "                if batch % validate_every == 0:\n",
    "                    validate(progress, validation_batches)  # Call validate function\n",
    "\n",
    "            progress.update(epoch_task, advance=1)\n",
    "            progress.remove_task(batch_task)\n",
    "\n",
    "\n",
    "# Run training\n",
    "train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "databundleVersionId": 290346,
     "isSourceIdPinned": false,
     "sourceId": 10737,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 30919,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "ML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
